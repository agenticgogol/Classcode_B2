{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a371398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/utsabchakraborty/Documents/Agentic_Class_Live/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "* Running on public URL: https://7be2a27fd768e33463.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://7be2a27fd768e33463.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OpenAI Vision Multimodal RAG (Phase 1)\n",
    "# ------------------------------------------------------------\n",
    "# A fully working, single-file demo that replaces Gemini with\n",
    "# OpenAI Vision and uses OpenAI embeddings + ChromaDB.\n",
    "#\n",
    "# Setup (recommended Python 3.10+):\n",
    "#   pip install gradio chromadb langchain pypdf pillow python-docx python-pptx \\\n",
    "#               langchain-openai openai PyPDF2\n",
    "#\n",
    "# Required environment variable or pass via UI:\n",
    "#   OPENAI_API_KEY\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "# Vector DB\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Chunking\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Docs parsing\n",
    "import PyPDF2\n",
    "from docx import Document\n",
    "from pptx import Presentation\n",
    "from PIL import Image\n",
    "\n",
    "# OpenAI (LLM + Embeddings)\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Multimodal RAG Implementation\n",
    "# -----------------------------\n",
    "class MultimodalRAG:\n",
    "    def __init__(self, openai_api_key: str, embedding_model: str = \"text-embedding-3-small\"):\n",
    "        if not openai_api_key:\n",
    "            raise ValueError(\"OpenAI API key is required\")\n",
    "\n",
    "        os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "        self.client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "        # Embeddings\n",
    "        try:\n",
    "            self.embeddings = OpenAIEmbeddings(\n",
    "                api_key=openai_api_key,\n",
    "                model=embedding_model,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to initialize OpenAIEmbeddings: {e}\")\n",
    "\n",
    "        # ChromaDB\n",
    "        try:\n",
    "            self.chroma_client = chromadb.PersistentClient(\n",
    "                path=\"./knowledge_base\",\n",
    "                settings=Settings(anonymized_telemetry=False, allow_reset=True),\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to initialize ChromaDB: {e}\")\n",
    "\n",
    "        # Collections\n",
    "        self.text_collection = self.chroma_client.get_or_create_collection(\n",
    "            name=\"text_documents\", metadata={\"description\": \"Text-based documents\"}\n",
    "        )\n",
    "        self.image_collection = self.chroma_client.get_or_create_collection(\n",
    "            name=\"image_documents\", metadata={\"description\": \"Image-derived content (via OpenAI Vision)\"}\n",
    "        )\n",
    "\n",
    "        # Chunker\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000, chunk_overlap=200, length_function=len\n",
    "        )\n",
    "\n",
    "        # Light session memory (in-memory)\n",
    "        self.session_memory: Dict[str, List[Dict]] = {}\n",
    "\n",
    "    # ---------\n",
    "    # Utilities\n",
    "    # ---------\n",
    "    @staticmethod\n",
    "    def _now_iso() -> str:\n",
    "        return datetime.now().isoformat()\n",
    "\n",
    "    # ---------------\n",
    "    # File Extraction\n",
    "    # ---------------\n",
    "    def extract_text_from_file(self, file_path: str, file_type: str) -> str:\n",
    "        try:\n",
    "            if file_type == \"pdf\":\n",
    "                text = \"\"\n",
    "                with open(file_path, \"rb\") as f:\n",
    "                    pdf_reader = PyPDF2.PdfReader(f)\n",
    "                    for page in pdf_reader.pages:\n",
    "                        # Some PDFs return None for empty pages; guard for that\n",
    "                        text += (page.extract_text() or \"\") + \"\\n\"\n",
    "                return text.strip()\n",
    "\n",
    "            elif file_type == \"docx\":\n",
    "                doc = Document(file_path)\n",
    "                return \"\\n\".join(p.text for p in doc.paragraphs)\n",
    "\n",
    "            elif file_type == \"pptx\":\n",
    "                prs = Presentation(file_path)\n",
    "                texts = []\n",
    "                for slide in prs.slides:\n",
    "                    for shape in slide.shapes:\n",
    "                        if hasattr(shape, \"text\"):\n",
    "                            texts.append(shape.text)\n",
    "                return \"\\n\".join(texts)\n",
    "\n",
    "            elif file_type == \"txt\":\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                    return f.read()\n",
    "\n",
    "            else:\n",
    "                return f\"Unsupported text file type: {file_type}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error extracting text: {e}\"\n",
    "\n",
    "    # ---------------------------\n",
    "    # Vision (OpenAI) for images\n",
    "    # ---------------------------\n",
    "    def _image_to_data_url(self, image_path: str) -> str:\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "        # Guess mime from ext\n",
    "        ext = os.path.splitext(image_path)[1].lower()\n",
    "        mime = {\n",
    "            \".png\": \"image/png\",\n",
    "            \".jpg\": \"image/jpeg\",\n",
    "            \".jpeg\": \"image/jpeg\",\n",
    "            \".gif\": \"image/gif\",\n",
    "            \".bmp\": \"image/bmp\",\n",
    "            \".webp\": \"image/webp\",\n",
    "        }.get(ext, \"image/png\")\n",
    "        return f\"data:{mime};base64,{b64}\"\n",
    "\n",
    "    def process_image_with_openai(self, image_path: str) -> str:\n",
    "        \"\"\"OCR + description + key info using GPT-4o-mini vision.\"\"\"\n",
    "        try:\n",
    "            data_url = self._image_to_data_url(image_path)\n",
    "            prompt = (\n",
    "                \"Analyze this image and provide:\\n\"\n",
    "                \"1) Any text visible (OCR)\\n\"\n",
    "                \"2) A detailed description\\n\"\n",
    "                \"3) Key information or concepts shown\\n\"\n",
    "                \"4) Context that might be useful for retrieval/search\\n\"\n",
    "                \"Return clear sections with headings.\"\n",
    "            )\n",
    "\n",
    "            # Using Chat Completions for broad compatibility\n",
    "            completion = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": prompt},\n",
    "                            {\"type\": \"image_url\", \"image_url\": {\"url\": data_url}},\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "            )\n",
    "            return completion.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            return f\"Error processing image with OpenAI Vision: {e}\"\n",
    "\n",
    "    # --------------------\n",
    "    # Ingestion / Indexing\n",
    "    # --------------------\n",
    "    def add_document(self, file_path: str, file_name: str, session_id: str = \"default\") -> str:\n",
    "        try:\n",
    "            ext = file_name.lower().split(\".\")[-1]\n",
    "\n",
    "            if ext in [\"pdf\", \"docx\", \"pptx\", \"txt\"]:\n",
    "                content = self.extract_text_from_file(file_path, ext)\n",
    "                if not content or content.startswith(\"Error\"):\n",
    "                    return f\"‚ùå Failed to extract content from {file_name}\"\n",
    "\n",
    "                chunks = self.text_splitter.split_text(content)\n",
    "                if not chunks:\n",
    "                    return f\"‚ùå No text found in {file_name}\"\n",
    "\n",
    "                # Embed in batches for efficiency\n",
    "                try:\n",
    "                    vectors = self.embeddings.embed_documents(chunks)\n",
    "                except Exception as e:\n",
    "                    return f\"‚ùå Embedding error for {file_name}: {e}\"\n",
    "\n",
    "                ids = []\n",
    "                metadatas = []\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    ids.append(f\"{file_name}_{i}_{int(datetime.now().timestamp())}\")\n",
    "                    metadatas.append(\n",
    "                        {\n",
    "                            \"file_name\": file_name,\n",
    "                            \"file_type\": ext,\n",
    "                            \"chunk_index\": i,\n",
    "                            \"session_id\": session_id,\n",
    "                            \"timestamp\": self._now_iso(),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                # Persist to Chroma\n",
    "                self.text_collection.add(\n",
    "                    embeddings=vectors,\n",
    "                    documents=chunks,\n",
    "                    metadatas=metadatas,\n",
    "                    ids=ids,\n",
    "                )\n",
    "\n",
    "                # Session memory\n",
    "                self.session_memory.setdefault(session_id, []).append(\n",
    "                    {\n",
    "                        \"file_name\": file_name,\n",
    "                        \"file_type\": ext,\n",
    "                        \"chunks_count\": len(chunks),\n",
    "                        \"timestamp\": self._now_iso(),\n",
    "                    }\n",
    "                )\n",
    "                return f\"‚úÖ Successfully processed {file_name} ({len(chunks)} chunks)\"\n",
    "\n",
    "            elif ext in [\"jpg\", \"jpeg\", \"png\", \"gif\", \"bmp\", \"webp\"]:\n",
    "                analysis = self.process_image_with_openai(file_path)\n",
    "                if not analysis or analysis.startswith(\"Error\"):\n",
    "                    return f\"‚ùå Failed to process image {file_name}: {analysis}\"\n",
    "\n",
    "                try:\n",
    "                    emb = self.embeddings.embed_query(analysis)\n",
    "                except Exception as e:\n",
    "                    return f\"‚ùå Embedding error for image {file_name}: {e}\"\n",
    "\n",
    "                doc_id = f\"{file_name}_{int(datetime.now().timestamp())}\"\n",
    "                self.image_collection.add(\n",
    "                    embeddings=[emb],\n",
    "                    documents=[analysis],\n",
    "                    metadatas=[\n",
    "                        {\n",
    "                            \"file_name\": file_name,\n",
    "                            \"file_type\": \"image\",\n",
    "                            \"session_id\": session_id,\n",
    "                            \"timestamp\": self._now_iso(),\n",
    "                        }\n",
    "                    ],\n",
    "                    ids=[doc_id],\n",
    "                )\n",
    "\n",
    "                self.session_memory.setdefault(session_id, []).append(\n",
    "                    {\n",
    "                        \"file_name\": file_name,\n",
    "                        \"file_type\": \"image\",\n",
    "                        \"timestamp\": self._now_iso(),\n",
    "                    }\n",
    "                )\n",
    "                return f\"‚úÖ Successfully processed image {file_name}\"\n",
    "            else:\n",
    "                return f\"‚ùå Unsupported file type: .{ext}\"\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Error processing {file_name}: {e}\"\n",
    "\n",
    "    # -------\n",
    "    # Search\n",
    "    # -------\n",
    "    def search_knowledge_base(self, query: str, session_id: str = \"default\", top_k: int = 5) -> List[Dict]:\n",
    "        try:\n",
    "            q_emb = self.embeddings.embed_query(query)\n",
    "\n",
    "            where_filter = {\"session_id\": session_id} if session_id in self.session_memory else None\n",
    "\n",
    "            text_results = self.text_collection.query(\n",
    "                query_embeddings=[q_emb],\n",
    "                n_results=max(1, top_k // 2),\n",
    "                where=where_filter,\n",
    "                include=[\"documents\", \"metadatas\", \"distances\"],\n",
    "            )\n",
    "            image_results = self.image_collection.query(\n",
    "                query_embeddings=[q_emb],\n",
    "                n_results=max(1, top_k // 2),\n",
    "                where=where_filter,\n",
    "                include=[\"documents\", \"metadatas\", \"distances\"],\n",
    "            )\n",
    "\n",
    "            def pack(res: Dict, typ: str) -> List[Dict]:\n",
    "                items = []\n",
    "                if res and res.get(\"documents\") and res[\"documents\"][0]:\n",
    "                    for i, doc in enumerate(res[\"documents\"][0]):\n",
    "                        items.append(\n",
    "                            {\n",
    "                                \"content\": doc,\n",
    "                                \"metadata\": res[\"metadatas\"][0][i],\n",
    "                                \"distance\": res[\"distances\"][0][i],\n",
    "                                \"type\": typ,\n",
    "                            }\n",
    "                        )\n",
    "                return items\n",
    "\n",
    "            all_results = pack(text_results, \"text\") + pack(image_results, \"image\")\n",
    "            all_results.sort(key=lambda x: x.get(\"distance\", 1.0))\n",
    "            return all_results[:top_k]\n",
    "        except Exception as e:\n",
    "            return [\n",
    "                {\n",
    "                    \"content\": f\"Search error: {e}\",\n",
    "                    \"metadata\": {},\n",
    "                    \"distance\": 1.0,\n",
    "                    \"type\": \"error\",\n",
    "                }\n",
    "            ]\n",
    "\n",
    "    # ------------------\n",
    "    # Answer Generation\n",
    "    # ------------------\n",
    "    def generate_answer(self, query: str, session_id: str = \"default\") -> str:\n",
    "        results = self.search_knowledge_base(query, session_id=session_id, top_k=6)\n",
    "        if not results or all(r.get(\"type\") == \"error\" for r in results):\n",
    "            return \"I couldn't find relevant information in your knowledge base. Please upload some documents first.\"\n",
    "\n",
    "        context_parts = []\n",
    "        sources = []\n",
    "        for i, r in enumerate(results):\n",
    "            if r.get(\"type\") != \"error\":\n",
    "                fn = r.get(\"metadata\", {}).get(\"file_name\", \"Unknown\")\n",
    "                snippet = (r.get(\"content\") or \"\")[:800]\n",
    "                context_parts.append(f\"Document {i+1} ({r['type']}) from {fn}:\\n{snippet}\\n---\")\n",
    "                sources.append(fn)\n",
    "\n",
    "        context_block = \"\\n\\n\".join(context_parts)\n",
    "        sys_prompt = (\n",
    "            \"You are a helpful enterprise knowledge assistant. Answer strictly based on the provided context. \"\n",
    "            \"If information is missing, clearly say what's missing. Provide concise, actionable answers and cite the source filenames inline like [Source: filename].\"\n",
    "        )\n",
    "\n",
    "        user_prompt = (\n",
    "            f\"Context:\\n{context_block}\\n\\n\"\n",
    "            f\"Question: {query}\\n\\n\"\n",
    "            \"Instructions:\\n\"\n",
    "            \"1) Use only the context above.\\n\"\n",
    "            \"2) If context is insufficient, say so and specify what is needed.\\n\"\n",
    "            \"3) Include short source attributions like [Source: <filename>].\\n\"\n",
    "        )\n",
    "\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.2,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "        )\n",
    "        answer = completion.choices[0].message.content.strip()\n",
    "\n",
    "        if sources:\n",
    "            answer += \"\\n\\nüìö Sources: \" + \", \".join(sorted(set(sources)))\n",
    "        return answer\n",
    "\n",
    "    # --------------\n",
    "    # Session status\n",
    "    # --------------\n",
    "    def get_session_info(self, session_id: str = \"default\") -> str:\n",
    "        if session_id not in self.session_memory:\n",
    "            return \"No documents uploaded in this session.\"\n",
    "        docs = self.session_memory[session_id]\n",
    "        info = f\"üìÅ **Session Documents ({len(docs)} files):**\\n\\n\"\n",
    "        for d in docs:\n",
    "            info += f\"‚Ä¢ {d['file_name']} ({d['file_type']}) - {d['timestamp'][:19]}\\n\"\n",
    "            if \"chunks_count\" in d:\n",
    "                info += f\"  ‚îî‚îÄ‚îÄ {d['chunks_count']} text chunks\\n\"\n",
    "        return info\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Gradio App / UI Wiring\n",
    "# -----------------------\n",
    "rag_system: MultimodalRAG | None = None\n",
    "\n",
    "\n",
    "def initialize_system(openai_key: str) -> str:\n",
    "    global rag_system\n",
    "    if not openai_key and not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        return \"‚ùå Please provide an OpenAI API key\"\n",
    "    key = openai_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "    try:\n",
    "        rag_system = MultimodalRAG(key)\n",
    "        return \"‚úÖ System initialized successfully!\"\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error initializing system: {e}\"\n",
    "\n",
    "\n",
    "def upload_document(files, session_id: str = \"default\") -> str:\n",
    "    if rag_system is None:\n",
    "        return \"‚ùå Please initialize the system with your API key first\"\n",
    "    if not files:\n",
    "        return \"‚ùå No files uploaded\"\n",
    "\n",
    "    results = []\n",
    "    for f in files:\n",
    "        # gr.Files gives tempfile objects with .name that points to a real path\n",
    "        results.append(rag_system.add_document(f.name, os.path.basename(f.name), session_id))\n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "\n",
    "def ask_question(question: str, session_id: str = \"default\") -> str:\n",
    "    if rag_system is None:\n",
    "        return \"‚ùå Please initialize the system with your API key first\"\n",
    "    if not question:\n",
    "        return \"‚ùå Please ask a question\"\n",
    "    return rag_system.generate_answer(question, session_id)\n",
    "\n",
    "\n",
    "def get_session_status(session_id: str = \"default\") -> str:\n",
    "    if rag_system is None:\n",
    "        return \"‚ùå System not initialized\"\n",
    "    return rag_system.get_session_info(session_id)\n",
    "\n",
    "\n",
    "# ----------------\n",
    "# Gradio Interface\n",
    "# ----------------\n",
    "\n",
    "def create_gradio_interface():\n",
    "    with gr.Blocks(title=\"Enterprise Knowledge Assistant ‚Äî OpenAI Vision\", theme=gr.themes.Soft()) as demo:\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            # üß† Enterprise Knowledge Assistant (OpenAI Vision)\n",
    "\n",
    "            Upload documents (PDF, DOCX, PPTX, TXT, Images) and ask questions about their content.\\\n",
    "            This app uses **OpenAI Vision (GPT-4o-mini)** for images, **OpenAI Embeddings** for retrieval, and **ChromaDB** for vector storage.\n",
    "\n",
    "            **Install:**\n",
    "            ```bash\n",
    "            pip install gradio chromadb langchain pypdf pillow python-docx python-pptx \\\\\n",
    "                        langchain-openai openai PyPDF2\n",
    "            ```\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        with gr.Tab(\"üîß Setup\"):\n",
    "            gr.Markdown(\"### Initialize the System\")\n",
    "            openai_key_input = gr.Textbox(\n",
    "                label=\"OpenAI API Key\",\n",
    "                placeholder=\"Enter your OpenAI API key here...\",\n",
    "                type=\"password\",\n",
    "            )\n",
    "            init_btn = gr.Button(\"Initialize System\", variant=\"primary\")\n",
    "            init_status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "            init_btn.click(fn=initialize_system, inputs=[openai_key_input], outputs=[init_status])\n",
    "\n",
    "        with gr.Tab(\"üìÅ Upload Documents\"):\n",
    "            gr.Markdown(\"### Upload Your Documents\")\n",
    "            session_input = gr.Textbox(label=\"Session ID\", value=\"default\", placeholder=\"Enter session ID (optional)\")\n",
    "            file_upload = gr.Files(\n",
    "                label=\"Upload Documents\",\n",
    "                file_count=\"multiple\",\n",
    "                file_types=[\n",
    "                    \".pdf\",\n",
    "                    \".docx\",\n",
    "                    \".pptx\",\n",
    "                    \".txt\",\n",
    "                    \".jpg\",\n",
    "                    \".jpeg\",\n",
    "                    \".png\",\n",
    "                    \".gif\",\n",
    "                    \".bmp\",\n",
    "                    \".webp\",\n",
    "                ],\n",
    "            )\n",
    "            upload_btn = gr.Button(\"Process Documents\", variant=\"primary\")\n",
    "            upload_status = gr.Textbox(label=\"Upload Status\", interactive=False, lines=6)\n",
    "            upload_btn.click(fn=upload_document, inputs=[file_upload, session_input], outputs=[upload_status])\n",
    "\n",
    "        with gr.Tab(\"ü§ñ Ask Questions\"):\n",
    "            gr.Markdown(\"### Query Your Knowledge Base\")\n",
    "            session_query = gr.Textbox(label=\"Session ID\", value=\"default\")\n",
    "            question_input = gr.Textbox(\n",
    "                label=\"Your Question\", placeholder=\"Ask anything about your uploaded documents...\", lines=3\n",
    "            )\n",
    "            ask_btn = gr.Button(\"Get Answer\", variant=\"primary\")\n",
    "            answer_output = gr.Textbox(label=\"Answer\", interactive=False, lines=12)\n",
    "            ask_btn.click(fn=ask_question, inputs=[question_input, session_query], outputs=[answer_output])\n",
    "\n",
    "        with gr.Tab(\"üìä Session Info\"):\n",
    "            gr.Markdown(\"### Session Status\")\n",
    "            session_status_input = gr.Textbox(label=\"Session ID\", value=\"default\")\n",
    "            status_btn = gr.Button(\"Check Status\")\n",
    "            status_output = gr.Textbox(label=\"Session Information\", interactive=False, lines=10)\n",
    "            status_btn.click(fn=get_session_status, inputs=[session_status_input], outputs=[status_output])\n",
    "\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            ### üîß Troubleshooting\n",
    "            1. Make sure you're in your virtual environment.\n",
    "            2. Install deps again if needed:\n",
    "               ```bash\n",
    "               pip install --upgrade chromadb openai langchain-openai gradio\n",
    "               ```\n",
    "            3. Set `OPENAI_API_KEY` in your environment or paste it in **Setup**.\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo = create_gradio_interface()\n",
    "    demo.launch(server_name=\"0.0.0.0\", server_port=7861, share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9531952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
