{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4e6fd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain_openai import OpenAIEmbeddings  # updated import\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import google.generativeai as genai\n",
    "import PyPDF2\n",
    "from docx import Document\n",
    "from pptx import Presentation\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "# ---------------------- Agent types ----------------------\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class AgentType(Enum):\n",
    "    RESEARCHER = \"researcher\"\n",
    "    SYNTHESIZER = \"synthesizer\"\n",
    "    FACT_CHECKER = \"fact_checker\"\n",
    "    FOLLOW_UP = \"follow_up\"\n",
    "    COORDINATOR = \"coordinator\"\n",
    "\n",
    "@dataclass\n",
    "class AgentResponse:\n",
    "    agent_type: AgentType\n",
    "    content: str\n",
    "    confidence: float\n",
    "    sources: List[str]\n",
    "    reasoning: str\n",
    "    follow_up_needed: bool = False\n",
    "\n",
    "# ---------------------- Vector DB wrapper ----------------------\n",
    "class VectorDB:\n",
    "    def __init__(self, name: str, metadata: dict = None):\n",
    "        self.name = name\n",
    "        self.metadata = metadata or {}\n",
    "\n",
    "    def add_documents(self, documents: List[str], metadatas: List[dict], ids: List[str], embeddings: List[List[float]]):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def query(self, embedding: List[float], n_results: int = 5):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_stats(self) -> dict:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "# ---------------------- Chroma Vector DB ----------------------\n",
    "class ChromaVectorDB(VectorDB):\n",
    "    def __init__(self, name: str, persist_path: str, metadata: dict = None, anonymized_telemetry: bool = False):\n",
    "        super().__init__(name, metadata)\n",
    "        os.makedirs(persist_path, exist_ok=True)\n",
    "        self.client = chromadb.PersistentClient(path=persist_path, settings=Settings(anonymized_telemetry=anonymized_telemetry))\n",
    "        self.collection = self.client.get_or_create_collection(name=name, metadata=metadata or {})\n",
    "        self.persist_path = persist_path\n",
    "\n",
    "    def add_documents(self, documents: List[str], metadatas: List[dict], ids: List[str], embeddings: List[List[float]]):\n",
    "        self.collection.add(\n",
    "            embeddings=embeddings,\n",
    "            documents=documents,\n",
    "            metadatas=metadatas,\n",
    "            ids=ids\n",
    "        )\n",
    "\n",
    "    def query(self, embedding: List[float], n_results: int = 5):\n",
    "        resp = self.collection.query(query_embeddings=[embedding], n_results=n_results)\n",
    "        return {\n",
    "            \"documents\": resp.get(\"documents\", [[]])[0] if isinstance(resp.get(\"documents\", None), list) else resp.get(\"documents\", []),\n",
    "            \"metadatas\": resp.get(\"metadatas\", [[]])[0] if isinstance(resp.get(\"metadatas\", None), list) else resp.get(\"metadatas\", []),\n",
    "            \"ids\": resp.get(\"ids\", [[]])[0] if isinstance(resp.get(\"ids\", None), list) else resp.get(\"ids\", []),\n",
    "            \"distances\": resp.get(\"distances\", [[]])[0] if isinstance(resp.get(\"distances\", None), list) else resp.get(\"distances\", [])\n",
    "        }\n",
    "\n",
    "    def get_stats(self) -> dict:\n",
    "        try:\n",
    "            count = self.collection.count()\n",
    "        except Exception:\n",
    "            count = None\n",
    "        return {\"name\": self.name, \"persist_path\": self.persist_path, \"count\": count}\n",
    "\n",
    "# ---------------------- VectorDB Registry ----------------------\n",
    "class VectorDBRegistry:\n",
    "    def __init__(self, embeddings: OpenAIEmbeddings):\n",
    "        self.embeddings = embeddings\n",
    "        self.dbs: Dict[str, VectorDB] = {}\n",
    "        self.db_topic_vectors: Dict[str, np.ndarray] = {}\n",
    "        self.db_topic_keywords: Dict[str, List[str]] = {}\n",
    "\n",
    "    def register_db(self, db: VectorDB, domain_keywords: List[str]):\n",
    "        if db.name in self.dbs:\n",
    "            raise ValueError(f\"DB name '{db.name}' already registered.\")\n",
    "        self.dbs[db.name] = db\n",
    "        self.db_topic_keywords[db.name] = domain_keywords.copy()\n",
    "        kw_embeddings = [self.embeddings.embed_query(k) for k in domain_keywords]\n",
    "        avg = np.mean(np.array(kw_embeddings, dtype=np.float32), axis=0)\n",
    "        self.db_topic_vectors[db.name] = avg / np.linalg.norm(avg)\n",
    "\n",
    "    def choose_dbs(self, query: str, top_k: int = 1) -> List[Tuple[str, float]]:\n",
    "        q_emb = np.array(self.embeddings.embed_query(query), dtype=np.float32)\n",
    "        q_emb_norm = q_emb / np.linalg.norm(q_emb)\n",
    "        sims = []\n",
    "        for name, topic_vec in self.db_topic_vectors.items():\n",
    "            sim = float(np.dot(q_emb_norm, topic_vec))\n",
    "            sims.append((name, sim))\n",
    "        sims_sorted = sorted(sims, key=lambda x: x[1], reverse=True)\n",
    "        return sims_sorted[:top_k]\n",
    "\n",
    "    def get_db(self, name: str) -> VectorDB:\n",
    "        return self.dbs[name]\n",
    "\n",
    "# ---------------------- Agentic RAG ----------------------\n",
    "class AgenticRAGMultiDB:\n",
    "    def __init__(self, gemini_api_key: str, openai_api_key: str):\n",
    "        genai.configure(api_key=gemini_api_key)\n",
    "        self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "        self.embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=openai_api_key)\n",
    "        self.registry = VectorDBRegistry(self.embeddings)\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)\n",
    "        self.session_memory: Dict[str, List[Dict[str, Any]]] = {}\n",
    "\n",
    "    def register_chroma_db(self, name: str, persist_path: str, domain_keywords: List[str]):\n",
    "        metadata = {\"created_by\": \"AgenticRAG\", \"db_name\": name}\n",
    "        db = ChromaVectorDB(name=name, persist_path=persist_path, metadata=metadata)\n",
    "        self.registry.register_db(db, domain_keywords)\n",
    "        return f\"✅ Registered Chroma DB '{name}' with keywords: {domain_keywords}\"\n",
    "\n",
    "    def ingest_to_db(self, file_path: str, file_name: str, target_db_name: Optional[str] = None, session_id: str = \"default\"):\n",
    "        ext = file_name.lower().split('.')[-1]\n",
    "        content = \"\"\n",
    "        try:\n",
    "            if ext == \"pdf\":\n",
    "                with open(file_path, \"rb\") as f:\n",
    "                    reader = PyPDF2.PdfReader(f)\n",
    "                    pages = [p.extract_text() or \"\" for p in reader.pages]\n",
    "                    content = \"\\n\".join(pages)\n",
    "            elif ext == \"docx\":\n",
    "                doc = Document(file_path)\n",
    "                content = \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "            elif ext == \"txt\":\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "            else:\n",
    "                return f\"❌ Unsupported extension: {ext}\"\n",
    "        except Exception as e:\n",
    "            return f\"❌ Error extracting {file_name}: {e}\"\n",
    "\n",
    "        if not target_db_name:\n",
    "            chosen = self.registry.choose_dbs(f\"{file_name} {content[:200]}\", top_k=1)\n",
    "            target_db_name = chosen[0][0] if chosen else list(self.registry.dbs.keys())[0]\n",
    "\n",
    "        db = self.registry.get_db(target_db_name)\n",
    "        chunks = self.text_splitter.split_text(content)\n",
    "        embeddings = [self.embeddings.embed_query(c) for c in chunks]\n",
    "        ids = [f\"{file_name}_{i}_{datetime.now().timestamp()}\" for i in range(len(chunks))]\n",
    "        metadatas = [{\"file_name\": file_name, \"file_type\": ext, \"chunk_index\": i, \"session_id\": session_id} for i in range(len(chunks))]\n",
    "        db.add_documents(chunks, metadatas, ids, embeddings)\n",
    "        return f\"✅ Ingested {file_name} into DB '{target_db_name}' ({len(chunks)} chunks).\"\n",
    "\n",
    "    def retrieve_for_query(self, query: str, top_dbs: int = 1, n_results_per_db: int = 3):\n",
    "        chosen = self.registry.choose_dbs(query, top_k=top_dbs)\n",
    "        docs_collected, sources = [], []\n",
    "        q_emb = self.embeddings.embed_query(query)\n",
    "        for db_name, sim in chosen:\n",
    "            db = self.registry.get_db(db_name)\n",
    "            results = db.query(q_emb, n_results=n_results_per_db)\n",
    "            retrieved_docs = results.get(\"documents\", [])\n",
    "            retrieved_meta = results.get(\"metadatas\", [])\n",
    "            for i, d in enumerate(retrieved_docs):\n",
    "                docs_collected.append(d)\n",
    "                meta = retrieved_meta[i] if i < len(retrieved_meta) else {}\n",
    "                sources.append(f\"{db_name}:{meta.get('file_name', 'unknown')}\")\n",
    "        return docs_collected, sources\n",
    "\n",
    "# ---------------------- Gradio App ----------------------\n",
    "rag_instance = None\n",
    "uploaded_files: List[str] = []\n",
    "\n",
    "def init_rag(gemini_key, openai_key):\n",
    "    global rag_instance, uploaded_files\n",
    "    rag_instance = AgenticRAGMultiDB(gemini_key, openai_key)\n",
    "    uploaded_files = []\n",
    "    # Example DBs\n",
    "    rag_instance.register_chroma_db(\"legal_db\", \"./chroma_legal\", [\"contracts\", \"agreements\", \"law\"])\n",
    "    rag_instance.register_chroma_db(\"finance_db\", \"./chroma_finance\", [\"finance\", \"revenue\", \"profit\"])\n",
    "    return \"✅ Agentic RAG initialized with 2 DBs.\"\n",
    "\n",
    "def upload_document(files):\n",
    "    global uploaded_files\n",
    "    uploaded_files = files  # store paths\n",
    "    messages = [f\"✅ Uploaded {os.path.basename(f)}\" for f in files]\n",
    "    return \"\\n\".join(messages)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Update ingest_documents function\n",
    "def ingest_documents():\n",
    "    ingestion_results = []\n",
    "    for file_path in uploaded_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        msg = rag_instance.ingest_to_db(file_path, file_name)\n",
    "        # Extract DB name and chunk count from message\n",
    "        if \"Ingested\" in msg:\n",
    "            try:\n",
    "                db_name = msg.split(\"into DB '\")[1].split(\"'\")[0]\n",
    "                chunk_count = int(msg.split(\"(\")[1].split()[0])\n",
    "            except Exception:\n",
    "                db_name = \"Unknown\"\n",
    "                chunk_count = 0\n",
    "        else:\n",
    "            db_name = \"Error\"\n",
    "            chunk_count = 0\n",
    "        ingestion_results.append({\n",
    "            \"File Name\": file_name,\n",
    "            \"Target DB\": db_name,\n",
    "            \"Chunks\": chunk_count,\n",
    "            \"Status\": msg\n",
    "        })\n",
    "    df = pd.DataFrame(ingestion_results)\n",
    "    return df\n",
    "\n",
    "\n",
    "def ask_query(query):\n",
    "    docs, sources = rag_instance.retrieve_for_query(query)\n",
    "\n",
    "    if not docs:\n",
    "        return \"No documents found.\"\n",
    "\n",
    "    # Concatenate chunks to feed to Gemini LLM\n",
    "    context = \"\\n\\n\".join(docs[:5])  # limit to top 5 chunks to avoid token limits\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant. Use the following document excerpts to answer the user's question.\n",
    "    Question: {query}\n",
    "    Documents:\n",
    "    {context}\n",
    "\n",
    "    Answer concisely, using only the relevant information from the documents.\n",
    "    \"\"\"\n",
    "\n",
    "    response = rag_instance.gemini_model.generate_content(prompt)\n",
    "    answer_text = getattr(response, \"text\", None) or \"No answer generated.\"\n",
    "\n",
    "    dbs_used = set([s.split(\":\")[0] for s in sources])\n",
    "    return f\"Answer (excerpt):\\n{answer_text}\\n\\nDB(s) used: {', '.join(dbs_used)}\"\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Agentic RAG Multi-DB Demo\")\n",
    "    with gr.Row():\n",
    "        gemini_key = gr.Textbox(label=\"Gemini API Key\", type=\"password\")\n",
    "        openai_key = gr.Textbox(label=\"OpenAI API Key\", type=\"password\")\n",
    "        btn_init = gr.Button(\"Initialize Agentic RAG\")\n",
    "        output_init = gr.Textbox(label=\"Initialization Status\")\n",
    "        btn_init.click(init_rag, inputs=[gemini_key, openai_key], outputs=output_init)\n",
    "    with gr.Row():\n",
    "        file_input = gr.Files(file_types=[\".pdf\", \".docx\", \".txt\"], label=\"Upload Documents\")\n",
    "        btn_upload = gr.Button(\"Upload Documents\")\n",
    "        output_upload = gr.Textbox(label=\"Upload Status\")\n",
    "        btn_upload.click(upload_document, inputs=file_input, outputs=output_upload)\n",
    "    with gr.Row():\n",
    "        btn_ingest = gr.Button(\"Ingest Uploaded Documents\")\n",
    "        output_ingest = gr.DataFrame(headers=[\"File Name\", \"Target DB\", \"Chunks\", \"Status\"])\n",
    "        btn_ingest.click(ingest_documents, inputs=[], outputs=output_ingest)\n",
    "\n",
    "    with gr.Row():\n",
    "        query_input = gr.Textbox(label=\"Ask a question\")\n",
    "        btn_query = gr.Button(\"Query\")\n",
    "        output_query = gr.Textbox(label=\"Answer & DB used\")\n",
    "        btn_query.click(ask_query, inputs=query_input, outputs=output_query)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9148208",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd00ca2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
